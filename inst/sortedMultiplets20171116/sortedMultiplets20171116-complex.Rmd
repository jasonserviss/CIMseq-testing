---
title: "sortedMultiplets20171116 with added complexity"
author: "Jason T. Serviss"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

Sorted multiplets were used evaluate the performance of multiple cost functions to understand their performance with added "complexity". In the case of the sorted multiplets, the "complexity"" is low compared to a case where a normal tissue is used. In normal tissue we expect a larger variety of cell types (~5-10), some of which are relativley similar to each other. In the case of the sorted multiplets, we have only 3 cell types and they are relativley quite different to each other. Although the sorted multiplets were ment to be an "easy", or ideal, situation in order to test the algorithm, the algorithms performance with the sorted multiplets may not accuratley reflect its performance with a complex tissue. 

In the case of a complex tissue, it is desireable that the alrorithm ers to the side of false negatives rather than false positives. In the case where we have two cell types each having two sub-cell types and, in reality, it is only one sub-cell type from each cell type that has a connection in the tissue, we would only want that connection to be detected. Despite this, without penalizing complexity in the algorithm, the cost may actually be lower to assign a half fraction to each cell subtype. This would result in a connection between all four cell subtypes instead of the desired outcome erflecting the ground truth.

At the heart of the algorithm is the cost function. Multiple cost functions, with and without an in-built penalty for complexity, have been tested using the sorted multiplets. Results indicate that penalizing complexity generally gives a reduced accuracy compared to cost functions which only measure a sum of absolute errors. It is hypothesized that this could be due to the fact that penalizing complexity in data with low complexity may not be a worthwhile approach. To evaluate this it was desired to generate a dataset with increased complexity, in an effort to mirror the increased complexity in a tissue, and re-evaluate the distance functions.

To accomplish this, the singlets corresponding to the sorted multiplets were utilized, with the exception that at the classification step k-means clustering was used to further sub-divide the classes into twice the amount. Thus, for each individual cell type, 2 classes were generated. Using k-means clustering, instead of randomly dividing the individual cell types for examples, allowes subtle true differences in the cell types to be detected and, thus, mirrors a complex tissue where subtypes of a individual cell type exist. 

This dataset was then used to test 3 cost functions which penalize complexity (bic, bic10, and bic10.2) and 2 that do not (distToSlice and distToSliceNorm). The distToSlice and distToSliceNorm cost functions were among those that had a high performance with the sorted multiplets dataset, i.e. a low complexity dataset.

```{r}
packages <- c(
    "sp.scRNAseq",
    "sp.scRNAseqData",
    "sp.scRNAseqTesting",
    "printr",
    "ggthemes",
    "tidyverse"
)
purrr::walk(packages, library, character.only = TRUE)
rm(packages)
```

```{r}
addComplexity <- function(uObj, k = 6, plot = TRUE) {

  #add complexity
  tsne <- getData(uObj, "tsne")
  km <- kmeans(tsne, k, iter.max = 100, nstart = 100)[[1]]
  km <- tibble(class = km, multiplet = names(km))
  
  #plot
  p <- c()
  if(plot) {
    p <- tsne %>%
    as.data.frame() %>%
    rownames_to_column(var = "multiplet") %>%
    as_tibble() %>%
    full_join(km, by = "multiplet") %>%
    mutate(class = parse_factor(class, levels = unique(class))) %>%
    ggplot() +
    geom_point(aes(x = V1, y = V2, colour = class))
  }
  
  #rename and add to spUnsupervised object
  newClass <- tibble(
    multiplet = rownames(tsne),
    classReal = getData(uObj, "classification")
  ) %>%
  full_join(km, by = "multiplet") %>%
  unite(combined, classReal, class) %>%
  pull(combined)
  
  classification(uObj) <- newClass
  return(list(uObj, p, km))
}

```

Run method and show complexity added tsne plot.
```{r, fig.align='center', fig.height=8, fig.width=10}
sng <- str_detect(colnames(countsSorted2), "^s")

#create counts objects
cObjSng <- spCounts(countsSorted2[, sng], countsSortedERCC2[, sng])
cObjMul <- spCounts(countsSorted2[, !sng], countsSortedERCC2[, !sng])

#spUnsupervised
uObj <- spUnsupervised(cObjSng)

#rename classes
positions <- str_extract(colnames(getData(cObjSng, "counts")), "...$")
newClass <- case_when(
  positions == "E03" ~ "A375", #what is this? sorting issue?
  positions %in% paste0(sort(rep(LETTERS[1:8], 4)), c("01", "02", "03", "04")) ~ "HOS",
  positions %in% paste0(sort(rep(LETTERS[1:8], 4)), c("05", "06", "07", "08")) ~ "HCT116",
  positions %in% paste0(sort(rep(LETTERS[1:8], 4)), c("09", "10", "11", "12")) ~ "A375",
  TRUE ~ "error"
)
corresp <- getData(uObj, "classification") %>%
  tibble(oldClass = ., newClass = newClass) %>%
  distinct()

classification(uObj) <- newClass

gm <- getData(uObj, "groupMeans")
colnames(gm) <- pull(corresp, newClass)[match(colnames(gm), pull(corresp, oldClass))]
groupMeans(uObj) <- gm

tm <- getData(uObj, "tsneMeans")
tm$classification <- pull(corresp, newClass)[match(tm$classification, pull(corresp, oldClass))]
tsneMeans(uObj) <- tm


#add complexity
tmp <- addComplexity(uObj,  k = 6, plot = TRUE)
uObjComplex <- tmp[[1]]
groupMeans(uObjComplex) <- averageGroupExpression(cObjSng, getData(uObjComplex, "classification"), FALSE)
newClasses <- tmp[[3]]

#plot
tmp[[2]]

#spSwarm
distFuns <- c("bic", "bic10", "bic10.2", "distToSlice", "distToSliceNorm")
sObjs <- map(distFuns, ~spSwarm(cObjMul, uObjComplex, distFun = .x, swarmsize = 50, maxiter = 10)) %>%
  setNames(distFuns)

#rename
renameFnx <- function(sObj) {
  mat <- getData(sObj, "spSwarm")
  colnames(mat) <- gsub("(.*)..$", "\\1", colnames(mat))
  sObj@spSwarm <- mat
  return(sObj)
}

sObjs <- map(sObjs, ~renameFnx(.x))
```

Setup plate.
```{r}
#setup information
#order everything by column
multiplets <- c(
  c(rep("A375-HCT116", 3), rep("HCT116-HOS", 2), rep("A375-HOS", 3)),
  c(rep("A375-HCT116", 3), rep("HCT116-HOS", 2), rep("A375-HOS", 3)),
  c(rep("A375-HCT116", 2), rep("HCT116-HOS", 3), rep("A375-HOS", 3)),
  c(rep("A375-HCT116", 2), rep("HCT116-HOS", 3), rep("A375-HOS", 3)),
  c(rep("A375-HCT116-HOS", 3), rep("HCT116-HCT116-HOS", 2), rep("A375-HOS-HOS", 3)),
  c(rep("A375-HCT116-HOS", 3), rep("HCT116-HCT116-HOS", 2), rep("A375-HOS-HOS", 3)),
  c(rep("A375-HCT116-HOS", 2), rep("HCT116-HCT116-HOS", 3), rep("A375-HOS-HOS", 3)),
  c(rep("A375-HCT116-HOS", 2), rep("HCT116-HCT116-HOS", 3), rep("A375-HOS-HOS", 3)),
  c(rep("A375-HOS-HOS-HOS", 3), rep("A375-A375-HCT116-HCT116", 2), rep("A375-A375-HCT116-HOS", 3)),
  c(rep("A375-HOS-HOS-HOS", 3), rep("A375-A375-HCT116-HCT116", 2), rep("A375-A375-HCT116-HOS", 3)),
  c(rep("A375-HOS-HOS-HOS", 2), rep("A375-A375-HCT116-HCT116", 3), rep("A375-A375-HCT116-HOS", 3)),
  c(rep("A375-HOS-HOS-HOS", 2), rep("A375-A375-HCT116-HCT116", 3), rep("A375-A375-HCT116-HOS", 3))
)

cols <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
rows <- LETTERS[1:8]
names <- paste0("m.NJB00204.", rep(rows, 12), sort(rep(cols, 8)))

#make plate data
plateData <- tibble(
  row = rep(rows, 12),
  column = sort(rep(cols, 8)),
  multipletName = names,
  multipletComposition = multiplets
) %>%
mutate(connections = str_split(multipletComposition, "-")) %>%
mutate(connections = {map(.$connections, ~combn(.x, 2))})

#subset plateData to include only multiplets in the results
plateData <- filter(plateData, multipletName %in% colnames(getData(cObjMul, "counts")))
known <- setupPlate(plateData)
```

Calculate results.
```{r}
calcResFull <- function(sObj, known, ec) {
  sObj %>% 
    checkResults(., known, edge.cutoff = ec) %>%
    mutate(cellsInWell = case_when(
      str_extract(multiplet, "..$") %in% c("01", "02", "03", "04") ~ 2L,
      str_extract(multiplet, "..$") %in% c("05", "06", "07", "08") ~ 3L,
      str_extract(multiplet, "..$") %in% c("09", "10", "11", "12") ~ 4L
  )) %>%
  select(multiplet, cellsInWell, data.detected:ACC)
}

res <- map(sObjs, ~calcResFull(.x, known, 0.2))
```

Show summary.
```{r}
resSummary <- function(result) {
  result %>%
  group_by(cellsInWell) %>%
  summarize(
    meanTPR = mean(TPR, na.rm = TRUE),
    meanTNR = mean(TNR, na.rm = TRUE),
    meanACC = mean(ACC, na.rm = TRUE)
  )
}

map_dfr(res, resSummary, .id = "costFun") %>%
  arrange(cellsInWell, costFun)
```

Results are presented per number of cells included in the multiplet and the mean metrics are shown for each distance function. Overall the results indicate that, despite having no correction for complexity, the distToSliceNorm function has the best performance on the dataset. Of those cost functions with a penalty for complexity, bic10 seems to perform best when considering overall accuracy (ACC). The true positive rate (TPR) is somewhat sacraficed for an increase in the true negative rate (TNR) in the majority of tested scenerios for all bic-related functions.  

Below we show fractions from the spSwarm optimization for all results.

```{r}
elements <- names(res)
fracNames <- colnames(printResults(res[[1]], TRUE, sObjs[[1]]))[3]

map(
  elements,
  function(x) {
    printResults(res[[x]], TRUE, sObjs[[x]]) %>%
      select(., -(data.detected:ACC)) %>%
      add_column(costFun = x)
  }
) %>%
  setNames(elements) %>%
  bind_rows() %>%
  select(multiplet, cellsInWell, data.expected, costFun, `frac (HOS, HCT116, HOS, A375, HCT116, A375)`) %>%
  spread(4, 5) %>%
  add_column(fractions = fracNames, .before = 4) %>%
  arrange(cellsInWell)
```

Results for each individual multiplet and each cost function are included in full below.  

```{r}
map(
  elements,
  function(x) {
    printResults(res[[x]], TRUE, sObjs[[x]]) %>%
      add_column(costFun = x, .before = 3)
  }
) %>%
  setNames(elements) %>%
  bind_rows() %>%
  arrange(multiplet)
```


```{r}
sessionInfo()
```

---
title: "distToSlice penalize complexity"
author: "Jason T. Serviss"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: no
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

## Introduction

This analysis is designed as a complement to previous analysis that identified the distToSliceNorm cost funtion as the most accurate of the tested cost functions. In this analysis we attempt to further refine the distToSliceNorm function and examine the effects of adding a penalty for complex models to the function. To begin we utilized the sorted multiplets data set and evaluate the performance of the distToSliceNorm function compared to multiple variants of the function. We subsequently generate and utilize a "complexity" dataset allowing us to judge the performance of said functions in situations that more closely mirror a complex tissue.

```{r, warnings = FALSE}
packages <- c(
    "sp.scRNAseq",
    "sp.scRNAseqData",
    "sp.scRNAseqTesting",
    "printr",
    "ggthemes",
    "tidyverse"
)
purrr::walk(packages, library, character.only = TRUE)
rm(packages)
```

## Set up functions. 
The variants of the distToSliceNorm cost function that are tested all have the same structure such that the sum of differences are calculated after the cell type means have been multiplied with the fractions. This sum is subsequently multiplied by a penalty which increases the cost as a function of the model complexity. The penalty calculation is  1 + (k/log(nCells)) * e) where: k is the model complexity (the number of fractions > 0), nCells is the number of cells in the multiplet (estimated by the fraction of ERCC counts), and e is epsilon which is a constant. The tested functions in this analysis explore the values of e and the effects on the accuracy of the result, as well as the reduction in model complexity. To test this a cost function, named "dtsnCellNum" was developed which accepts e and nCells as arguments.

```{r}

.makeSyntheticSlice <- function(
    cellTypes,
    fractions
){
    return(colSums(t(cellTypes) * fractions))
}

.complexityPenilty <- function(k, e, cellNumber) {
  n <- k / log(round(cellNumber) + 0.1)
  u <- n * e
  1 + u
}

dtsnCellNum <- function(
    fractions,
    cellTypes,
    oneMultiplet,
    cellNumber,
    e,
    ...
){
  if(sum(fractions) == 0) {
      return(999999999)
  }
  normFractions <- fractions / sum(fractions)
  cellTypes <- cellTypes/mean(cellTypes)
  a = .makeSyntheticSlice(cellTypes, normFractions)
  a <- a/mean(a)
  penalty <- .complexityPenilty(length(which(normFractions > 0)), e, cellNumber)
  sum(abs((oneMultiplet - a) / (a+1))) * penalty
}

dtsnCellNumMod1 <- function(
    fractions,
    cellTypes,
    oneMultiplet,
    cellNumber,
    e,
    ...
){
  if(sum(fractions) == 0) {
      return(999999999)
  }
  normFractions <- fractions / sum(fractions)
  #cellTypes <- cellTypes/mean(cellTypes)
  cellTypes <- t(apply(cellTypes, 1, function(x) x / sum(x)))
  a = .makeSyntheticSlice(cellTypes, normFractions)
  a <- a/mean(a)
  penalty <- .complexityPenilty(length(which(normFractions > 0)), e, cellNumber)
  sum(abs((oneMultiplet - a) / (a+1))) * penalty
}

dtsnCellNumMod2 <- function(
    fractions,
    cellTypes,
    oneMultiplet,
    cellNumber,
    e,
    ...
){
  if(sum(fractions) == 0) {
      return(999999999)
  }
  normFractions <- fractions / sum(fractions)
  #cellTypes <- cellTypes/mean(cellTypes)
  cellTypes <- t(apply(cellTypes, 1, function(x) x / sum(x)))
  a = .makeSyntheticSlice(cellTypes, normFractions)
  #a <- a/mean(a)
  penalty <- .complexityPenilty(length(which(normFractions > 0)), e, cellNumber)
  sum(abs((oneMultiplet - a) / (a+1))) * penalty
}

###### Analysis functions
#The function calls the checkResults function and performes some downstream annotation of additional information.
calcResFull <- function(sObj, known, ec) {
  sObj %>% 
    checkResults(., known, edge.cutoff = ec) %>%
    mutate(cellsInWell = case_when(
      str_extract(multiplet, "..$") %in% c("01", "02", "03", "04") ~ 2L,
      str_extract(multiplet, "..$") %in% c("05", "06", "07", "08") ~ 3L,
      str_extract(multiplet, "..$") %in% c("09", "10", "11", "12") ~ 4L
  )) %>%
  select(multiplet, cellsInWell, data.detected:ACC)
}

#Takes output from the calcResFull function and provides an average for the TPR, TNR, and ACC metrics for each cost function.
resSummaryTotal <- function(result) {
  result %>%
  summarize(
    meanTPR = mean(TPR, na.rm = TRUE),
    meanTNR = mean(TNR, na.rm = TRUE),
    meanACC = mean(ACC, na.rm = TRUE)
  )
}

#Same as above except results are also calculated per number of cells in the multiplet.
resSummaryCells <- function(result) {
  result %>%
  group_by(cellsInWell) %>%
  summarize(
    meanTPR = mean(TPR, na.rm = TRUE),
    meanTNR = mean(TNR, na.rm = TRUE),
    meanACC = mean(ACC, na.rm = TRUE)
  )
}

#Adds complexity to the dataset by reclassifying the tsne into k groups using k-means clustering.
addComplexity <- function(uObj, k = 6, plot = TRUE) {

  #add complexity
  tsne <- getData(uObj, "tsne")
  km <- kmeans(tsne, k, iter.max = 100, nstart = 100)[[1]]
  km <- tibble(class = km, multiplet = names(km))
  
  #plot
  p <- c()
  if(plot) {
    p <- tsne %>%
    as.data.frame() %>%
    rownames_to_column(var = "multiplet") %>%
    as_tibble() %>%
    full_join(km, by = "multiplet") %>%
    mutate(class = parse_factor(class, levels = unique(class))) %>%
    ggplot() +
    geom_point(aes(x = V1, y = V2, colour = class))
  }
  
  #rename and add to spUnsupervised object
  newClass <- tibble(
    multiplet = rownames(tsne),
    classReal = getData(uObj, "classification")
  ) %>%
  full_join(km, by = "multiplet") %>%
  unite(combined, classReal, class) %>%
  pull(combined)
  
  classification(uObj) <- newClass
  return(list(uObj, p, km))
}
```

## Run method using normal (non-complex) data i.e. just the sorted multiplets.

We start by using the sorted multiplets data. Due to the fact that this may be considered "non-complex" data we expect that penalizing complexity may decrease the accuracy of the results. The goal is to adjust the e argument such that we find a balance between the accuracy in both non-complex and complex data.

```{r, fig.align='center', fig.height=8, fig.width=10}
#run the method
sng <- str_detect(colnames(countsSorted2), "^s")
cObjSng <- spCounts(countsSorted2[, sng], countsSortedERCC2[, sng])
cObjMul <- spCounts(countsSorted2[, !sng], countsSortedERCC2[, !sng])
uObj <- spUnsupervised(cObjSng)

#rename classes
classification(uObj) <- countsSortedMeta2$cellTypes[match(rownames(getData(uObj, "tsne")), countsSortedMeta2$sample)]
groupMeans(uObj) <- averageGroupExpression(cObjSng, getData(uObj, "classification"), TRUE, getData(uObj, "uncertainty"))
tsneMeans(uObj) <- tsneGroupMeans(getData(uObj, "tsne"), getData(uObj, "classification"))

distFuns <- list(
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNumMod1 = dtsnCellNumMod1,
  dtsnCellNumMod2 = dtsnCellNumMod2
)
e <- c(0.0025, 0.0025, 0.0025)
eNames <- c(0.0025, 0.0025, 0.0025)
distNames <- paste(names(distFuns), eNames, sep = "_")
distNames <- str_replace(distNames, "(.*)_$", "\\1")

cellNumber <- estimateCells(cObjSng, cObjMul)

sObjs <- map(
  1:length(distFuns), 
  ~spSwarm(
    cObjMul, 
    uObj, 
    distFun = distFuns[[.x]], 
    swarmsize = 50, 
    maxiter = 10, 
    cellNumber = cellNumber, 
    e = e[.x]
  )
) %>%
  setNames(distNames)

```

#spSwarm
distFuns <- list(
  distToSlice = sp.scRNAseq:::distToSlice, 
  distToSliceNorm = sp.scRNAseq:::distToSliceNorm, 
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNum = sp.scRNAseq:::dtsnCellNum,
  dtsnCellNumMod = dtsnCellNumMod
)
e <- c(NA, NA, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.1, 0.025)
eNames <- c("", "", 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.1, 0.025)
distNames <- paste(names(distFuns), eNames, sep = "_")
distNames <- str_replace(distNames, "(.*)_$", "\\1")

cellNumber <- estimateCells(cObjSng, cObjMul)

sObjs <- map(
  1:length(distFuns), 
  ~spSwarm(
    cObjMul, 
    uObj, 
    distFun = distFuns[[.x]], 
    swarmsize = 50, 
    maxiter = 10, 
    cellNumber = cellNumber, 
    e = e[.x]
  )
) %>%
  setNames(distNames)


```{r, fig.align='center', fig.height=6, fig.width=8, eval = FALSE}
# Plot the costs for each distance function.
map_dfr(sObjs, .f = ~getData(.x, "costs") %>% as_tibble(), .id = "costFun") %>%
  mutate(costFun = parse_factor(costFun, levels = distNames)) %>%
  ggplot() +
  geom_boxplot(aes(x = costFun, y = value)) +
  theme_few() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(y = "Cost")
```

```{r}
#Setup known multiplet compositions.
#setup information
#order everything by column
multiplets <- c(
  c(rep("A375-HCT116", 3), rep("HCT116-HOS", 2), rep("A375-HOS", 3)),
  c(rep("A375-HCT116", 3), rep("HCT116-HOS", 2), rep("A375-HOS", 3)),
  c(rep("A375-HCT116", 2), rep("HCT116-HOS", 3), rep("A375-HOS", 3)),
  c(rep("A375-HCT116", 2), rep("HCT116-HOS", 3), rep("A375-HOS", 3)),
  c(rep("A375-HCT116-HOS", 3), rep("HCT116-HCT116-HOS", 2), rep("A375-HOS-HOS", 3)),
  c(rep("A375-HCT116-HOS", 3), rep("HCT116-HCT116-HOS", 2), rep("A375-HOS-HOS", 3)),
  c(rep("A375-HCT116-HOS", 2), rep("HCT116-HCT116-HOS", 3), rep("A375-HOS-HOS", 3)),
  c(rep("A375-HCT116-HOS", 2), rep("HCT116-HCT116-HOS", 3), rep("A375-HOS-HOS", 3)),
  c(rep("A375-HOS-HOS-HOS", 3), rep("A375-A375-HCT116-HCT116", 2), rep("A375-A375-HCT116-HOS", 3)),
  c(rep("A375-HOS-HOS-HOS", 3), rep("A375-A375-HCT116-HCT116", 2), rep("A375-A375-HCT116-HOS", 3)),
  c(rep("A375-HOS-HOS-HOS", 2), rep("A375-A375-HCT116-HCT116", 3), rep("A375-A375-HCT116-HOS", 3)),
  c(rep("A375-HOS-HOS-HOS", 2), rep("A375-A375-HCT116-HCT116", 3), rep("A375-A375-HCT116-HOS", 3))
)

cols <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
rows <- LETTERS[1:8]
names <- paste0("m.NJB00204.", rep(rows, 12), sort(rep(cols, 8)))

#make plate data
plateData <- tibble(
  row = rep(rows, 12),
  column = sort(rep(cols, 8)),
  multipletName = names,
  multipletComposition = multiplets
) %>%
mutate(connections = str_split(multipletComposition, "-")) %>%
mutate(connections = {map(.$connections, combn, 2)})

plateData <- filter(plateData, multipletName %in% colnames(getData(cObjMul, "counts")))
known <- setupPlate(plateData)
```

```{r}
#Calculate results.
res <- map(sObjs, ~calcResFull(.x, known, 0))
```

### Show a summary of the results.

```{r}
map_dfr(res, resSummaryTotal, .id = "costFun") %>%
  mutate(costFun = parse_factor(costFun, levels = distNames)) %>%
  arrange(costFun)
```

Despite the fact that many of the cost functions have a high level of performance, overall those with the best accuracy have an e value of .00075 and .001. showing a average accuracy of ~93%. Generally, all e values between 1e-04 and 0.005 have an average accuracy of >90%.  


### Show a summary of the results divided by number of cells in the multiplets.

```{r}
map_dfr(res, resSummaryCells, .id = "costFun") %>%
  mutate(costFun = parse_factor(costFun, levels = distNames)) %>%
  arrange(cellsInWell, costFun)
```

The results above indicate that the cost functions differ somewhat in their accuracy dependant on the number of cells included in the multiplet.  

2 cells: For multiplets with 2 cells, e values < 5e-04 give the same accuracy as distToSliceNorm. With e = 0.0025, the accuracy is reduced but provides the highest level of accuracy of those cost functions that give a reduced accuracy when compared to distToSliceNorm. All values of e <= 0.005 give >90% average accuracy.    

3 cells: e values between 1e-04 and 0.0025 report identical average accuracy whereas average accuracy begins to decline with e values > 0.0025. Average accuracy for e values between 1e-04 and 0.0025 is > 90%.  

4 cells: Average accuracy for e values < 5e-04 are identical to the accuracy reported for distToSliceNorm. Interestingly, e values between 0.00075 and 0.005 have an increased average accuracy due to an increase in the true positive rate (TPR). In addition, all e values between 0.00075 and 0.005 have an average reported accuracy >90%.  

### Conclusion

With the sorted multiplet dataset we see that e values > 0.0025 tend to decrease the average accuracy. In the case of 2 cells in the multiplet, e value 0.0025 has a small decrease in accuracy compared to the highest reported average accuracy (decrease of ~3%) although the average accuracy is still quite high (~94%). In all other compositions of multiplet cell numbers, e value = 0.0025 is amoung those with the highest reported accuracy and, thus, the results indicate that e = 0.0025 allows the best tradeoff available to report a highly accurate result in cases where multiplets contain 2-4 cells.

## Run method using complex data.

Sorted multiplets were used evaluate the performance of multiple cost functions to understand their performance with added "complexity". In the case of the sorted multiplets, the "complexity"" is low compared to a case where a normal tissue is used. In normal tissue we expect a larger variety of cell types (~5-10), some of which are relativley similar to each other. In the case of the sorted multiplets, we have only 3 cell types and they are relativley different to each other. Although the sorted multiplets were ment to be an "easy", or ideal, situation in order to test the algorithm, the algorithms performance with the sorted multiplets may not accuratley reflect its performance with a complex tissue. 

In the case of a complex tissue, it is desireable that the alrorithm ers to the side of false negatives rather than false positives. In the case where we have two cell types each having two sub-cell types and, in reality, it is only one sub-cell type from each cell type that has a connection in the tissue, we would only want that connection to be detected. Despite this, without penalizing complexity in the algorithm, the cost may actually be lower to assign a half fraction to each cell subtype. This would result in a connection between all four cell subtypes instead of the desired outcome reflecting the ground truth.

At the heart of the algorithm is the cost function. Multiple cost functions, with and without an in-built penalty for complexity, have been tested using the sorted multiplets. Results indicate that penalizing complexity generally gives a reduced accuracy compared to cost functions which only measure a sum of absolute errors. It is hypothesized that this could be due to the fact that penalizing complexity in data with low complexity may not be a worthwhile approach. To evaluate this it was desired to generate a dataset with increased complexity, in an effort to mirror the increased complexity in a tissue, and re-evaluate the distance functions.

The results in the above section indicate that several of the tested cost functions are capable of reporting results with a high level of accuracy when tested on a "non-complex" dataset. In order to test which of these functions is also capable of effectivley penalizing a complex model without sacraficing accuracy we developed a "complex" dataset. To do this, the singlets corresponding to the sorted multiplets were utilized, with the exception that at the classification step k-means clustering was used to further sub-divide the classes into twice the amount. Thus, for each individual cell type, 2 classes were generated. Using k-means clustering, instead of randomly dividing the individual cell types for examples, allowes subtle true differences in the cell types to be detected and, thus, mirrors a complex tissue where subtypes of a individual cell type exist. This dataset was then used to test the same cost functions which were tested above. 

```{r, fig.align='center', fig.height=6, fig.width=8}
#add complexity
tmp <- addComplexity(uObj,  k = 6, plot = TRUE)
uObjComplex <- tmp[[1]]
groupMeans(uObjComplex) <- averageGroupExpression(cObjSng, getData(uObjComplex, "classification"), FALSE)
newClasses <- tmp[[3]]

#plot
tmp[[2]]

#spSwarm
sObjsComplex <- map(
  1:length(distFuns), 
  ~spSwarm(
    cObjMul, 
    uObjComplex, 
    distFun = distFuns[[.x]], 
    swarmsize = 50, 
    maxiter = 10, 
    cellNumber = cellNumber, 
    e = e[.x]
  )
) %>%
  setNames(distNames)

#rename (necessary for accurate downstream calculation of metrics with the calcResFull function)
renameFnx <- function(sObj) {
  mat <- getData(sObj, "spSwarm")
  colnames(mat) <- gsub("(.*)..$", "\\1", colnames(mat))
  sObj@spSwarm <- mat
  return(sObj)
}

sObjsComplex <- map(sObjsComplex, ~renameFnx(.x))
```

### Show the complexity of the results per cost function.

```{r, fig.align='center', fig.height=6, fig.width=8}
map_dfr(sObjsComplex, function(x) {
  swarm <- getData(x, "spSwarm")
  map_int(1:nrow(swarm), function(i) length(which(swarm[i, ] > 0))) %>%
    as_tibble()
}, .id = "costFun") %>%
  mutate(costFun = parse_factor(costFun, levels = distNames)) %>%
  ggplot() +
  geom_boxplot(aes(x = costFun, y = value)) +
  theme_few() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(y = "Complexity")

```

The plot above shows a negative correlation between the e value and the complexity of the result indicating that the cost function design is working as expected.

# Actual vs expected complexity

```{r}
#Calculate results.
resComplex <- map(sObjsComplex, ~calcResFull(.x, known, 0))
```

### Show a summary of the results.

```{r}
map_dfr(resComplex, resSummaryTotal, .id = "costFun") %>%
  mutate(costFun = parse_factor(costFun, levels = distNames)) %>%
  arrange(costFun)
```

Overall the cost functions with the highest reported accuracy have an e value of between 0.00075 and 0.001 although all result with an e value < 0.005 have an average accuracy > 90%. The results for e value 0.0025, which showed good results in the previous analysis, has a slight decrease in accuracy from the highest reported accuracy (-0.6%).

### Show a summary of the results divided by number of cells in the multiplets.

```{r}
map_dfr(resComplex, resSummaryCells, .id = "costFun") %>%
  mutate(costFun = parse_factor(costFun, levels = distNames)) %>%
  arrange(cellsInWell, costFun)
```

2 cells: Here e value 0.0025 out performs all other e values with an average accuracy of >95%. All e values < 0.0075 have an average accuracy > 90%.  

3 cells: Again we see that e value 0.0025 outperforms all other e values with an average accuracy of > 91%. Only e values between 0.001 and 0.0025 give an average accuracy > 90%.  

4 cells: A positive correlation between average accuracy and e value is observed for e values from 1e-4 to 0.005, afterwhich average accuracy begins to decrease. The highest average accuracy is observed with e value 0.005 (>89%) with e value 0.0025 trailing slightly after (88%).  

## Conclusion

Overall the e value of 0.0025 seems to give a good balance in accuracy in non-complex and complex data with multiplets that contain 2-4 cells. It should be noted that increases in swarm size and maximum iterations during swarm optimization may effect this result somewhat.

## Full data.

Included below is all results pertaining to the analyses.

### Fractions for all results.

#### Normal (non-complex) data from the sorted multiplets

```{r}
elements <- names(res)
fracNames <- colnames(printResults(res[[1]], TRUE, sObjs[[1]]))[3]

map(
  elements,
  function(x) {
    printResults(res[[x]], TRUE, sObjs[[x]]) %>%
      select(., -(data.detected:ACC)) %>%
      add_column(costFun = x)
  }
) %>%
  setNames(elements) %>%
  bind_rows() %>%
  select(multiplet, cellsInWell, data.expected, costFun, `frac (HOS, HCT116, A375)`) %>%
  spread(4, 5) %>%
  add_column(fractions = fracNames, .before = 4) %>%
  arrange(cellsInWell)
```

#### Complex data

```{r}
elementsComplex <- names(resComplex)
fracNamesComplex <- colnames(printResults(resComplex[[1]], TRUE, sObjsComplex[[1]]))[3]

map(
  elementsComplex,
  function(x) {
    printResults(resComplex[[x]], TRUE, sObjsComplex[[x]]) %>%
      select(., -(data.detected:ACC)) %>%
      add_column(costFun = x)
  }
) %>%
  setNames(elementsComplex) %>%
  bind_rows() %>%
  select(multiplet, cellsInWell, data.expected, costFun, `frac (HOS, HCT116, HOS, A375, HCT116, A375)`) %>%
  spread(4, 5) %>%
  add_column(fractions = fracNamesComplex, .before = 4) %>%
  arrange(cellsInWell)
```

### Results for each individual multiplet and each cost function.  

#### Normal (non-complex) data from the sorted multiplets

```{r}
map(
  elements,
  function(x) {
    printResults(res[[x]], TRUE, sObjs[[x]]) %>%
      add_column(costFun = x, .before = 3)
  }
) %>%
  setNames(elements) %>%
  bind_rows() %>%
  arrange(multiplet)
```

#### Complex data

```{r}
map(
  elementsComplex,
  function(x) {
    printResults(resComplex[[x]], TRUE, sObjsComplex[[x]]) %>%
      add_column(costFun = x, .before = 3)
  }
) %>%
  setNames(elementsComplex) %>%
  bind_rows() %>%
  arrange(multiplet)
```


```{r}
sessionInfo()
```
